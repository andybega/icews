---
title: "Speed comparison database versus file-based"
author: "Andreas Beger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Speed comparison database versus file-based}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Generally, in-memory calculations are much faster than calculations via SQL, but they come at the cost of loading several GB into memory, which on my laptop takes about 2 minutes. 

_(The values in this note were last updated on 2020-05-22. Since the SQL queries take several minutes to run, and depend on a working local database and files that are not available on CI, the code cells are mostly not evaluated when rendering this vignette.)_

```{r}
library(icews)
library(dplyr)
library(tidyr)
library(tictoc)
```

# Read into memory

```{r, results='hold', eval=FALSE}
tic()
events <- read_icews(find_db())
nrow(events)
format(object.size(events), "Gb")
toc()
```

```
#> [1] 18465057
#> [1] "2.9 Gb"
#> 165.218 sec elapsed
```

```{r, results='hold', eval=FALSE}
tic()
events <- read_icews(find_raw())
nrow(events)
format(object.size(events), "Gb")
toc()
```

```
#> [1] 18465057
#> [1] "2.8 Gb"
#> 155.476 sec elapsed
```

# Country-year event counts

The in-memory calculation is much faster than in the database, although longer if one counts the overhead of reading the data into memory as well.

```{r, results='hold', eval=FALSE}
tic()
cy_total_mem <- events %>% 
  group_by(country, year) %>% 
  summarize(events = n())
toc()
```

```
#> 2.313 sec elapsed
```

And here by querying the database:

```{r, eval=FALSE}
tic()
cy_total_db <- query_icews(
  "SELECT count(*) AS total FROM events GROUP BY country, year;")
toc()
```

```
#> 740.567 sec elapsed
```

Equivalent results, but much faster:

```{r, eval=FALSE}
tic()
cy_total_db <- query_icews(
  "SELECT count() AS total FROM events GROUP BY year, country;")
toc()
```

```
#> 52.795 sec elapsed
```

This is an equivalent SQL query but using **dplyr**'s behind the scenes construction:

```{r, eval=FALSE}
tic()
cy_total_db <- tbl(connect(), "events") %>% 
  group_by(year, country) %>% 
  summarize(events = n()) %>%
  collect()
toc()
```

```r
#> 54.855 sec elapsed
```

# Column cardinality

Usually SQL indices work better on columns that have high cardinality.

```{r, eval=FALSE}
col_vals <- query_icews("
select count(*) as rows,
       count(distinct(event_id)) as event_id,
       count(distinct(event_date)) as event_date,
       count(distinct(source_name)) as source_name,
       count(distinct(source_sectors)) as source_sectors,
       count(distinct(source_country)) as source_country,
       count(distinct(event_text)) as event_text,
       count(distinct(cameo_code)) as cameo_code,
       count(distinct(intensity)) as intensity,
       count(distinct(target_name)) as target_name,
       count(distinct(target_sectors)) as target_sectors,
       count(distinct(target_country)) as target_country,
       count(distinct(story_id)) as story_id,
       count(distinct(sentence_number)) as sentence_number,
       count(distinct(publisher)) as publisher,
       count(distinct(city)) as city,
       count(distinct(district)) as district,
       count(distinct(province)) as province,
       count(distinct(country)) as country,
       count(distinct(latitude)) as latitude,
       count(distinct(longitude)) as longitude,
       count(distinct(year)) as year,
       count(distinct(yearmonth)) as yearmonth,
       count(distinct(source_file)) as source_file
from events;") 

col_vals <- col_vals %>%
  tidyr::pivot_longer(rows:source_file, 
                      names_to = "Column", 
                      values_to = "Unique_values") %>%
  arrange(Unique_values, Column) 

col_vals %>%
  knitr::kable()
```

|Column          | Unique_values|
|:---------------|-------------:|
|sentence_number |             6|
|year            |            26|
|source_file     |            29|
|intensity       |            41|
|source_country  |           250|
|target_country  |           250|
|country         |           251|
|cameo_code      |           273|
|event_text      |           273|
|publisher       |           290|
|yearmonth       |           305|
|province        |          5263|
|event_date      |          9269|
|district        |         10055|
|target_name     |         74604|
|source_name     |         76982|
|latitude        |         88440|
|city            |         91348|
|longitude       |         92562|
|target_sectors  |        222589|
|source_sectors  |        276177|
|story_id        |       9040119|
|event_id        |      18319702|
|rows            |      18465014|

No indices are created by default, but if you prefer working with the events in a SQLite database and doing certain queries frequently then it may make sense to create some. There are plenty of tutorials online on how to create good indices (for example [_Use the index, Luke!_](https://use-the-index-luke.com)), and you can always preface a query you are developing with `EXPLAIN QUERY PLAN [query]` (see [here](https://www.sqlite.org/eqp.html) for details).
